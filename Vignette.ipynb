{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoFillGluon: Machine Learning-Based Missing Data Imputation\n",
    "\n",
    "This vignette demonstrates how to use the AutoFillGluon package for advanced imputation of missing data. AutoFillGluon leverages the power of AutoGluon to provide sophisticated, machine learning-based imputation for both numerical and categorical variables.\n",
    "\n",
    "## Overview\n",
    "\n",
    "AutoFillGluon offers the following key features:\n",
    "- ML-based imputation using AutoGluon's predictive models\n",
    "- Iterative refinement for improved quality\n",
    "- Handles both numerical and categorical data\n",
    "- Multiple imputation support\n",
    "- Built-in evaluation of imputation quality\n",
    "- Integration with survival analysis via custom scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll install the required packages and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "#!pip install autogluon pandas numpy scikit-learn matplotlib seaborn lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import AutoFillGluon components\n",
    "from autofillgluon import Imputer, multiple_imputation\n",
    "from autofillgluon.utils import calculate_missingness_statistics, plot_imputation_evaluation\n",
    "from autofillgluon import concordance_index_scorer, cox_ph_scorer, exponential_nll_scorer\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Imputation with Synthetic Data\n",
    "\n",
    "Let's start with a simple example using synthetic data to demonstrate the core functionality of the imputation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset with missing values:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "income",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "experience",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "satisfaction",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "department",
         "rawType": "category",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2ca1959c-7d79-4f92-8429-d9b2f3c36d31",
       "rows": [
        [
         "0",
         "44.96714153011233",
         "63499.396420099794",
         "20.746339142487923",
         "High",
         "Marketing"
        ],
        [
         "1",
         "38.61735698828815",
         null,
         null,
         "Low",
         "Sales"
        ],
        [
         "2",
         "46.47688538100692",
         "55242.978269895015",
         "13.545293177003082",
         "Medium",
         "Engineering"
        ],
        [
         "3",
         "55.23029856408026",
         null,
         "16.816322425649663",
         "High",
         "Engineering"
        ],
        [
         "4",
         "37.658466252766644",
         "44733.10800363646",
         "12.445494618993626",
         null,
         null
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>experience</th>\n",
       "      <th>satisfaction</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.967142</td>\n",
       "      <td>63499.396420</td>\n",
       "      <td>20.746339</td>\n",
       "      <td>High</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.617357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.476885</td>\n",
       "      <td>55242.978270</td>\n",
       "      <td>13.545293</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.230299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.816322</td>\n",
       "      <td>High</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.658466</td>\n",
       "      <td>44733.108004</td>\n",
       "      <td>12.445495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age        income  experience satisfaction   department\n",
       "0  44.967142  63499.396420   20.746339         High    Marketing\n",
       "1  38.617357           NaN         NaN          Low        Sales\n",
       "2  46.476885  55242.978270   13.545293       Medium  Engineering\n",
       "3  55.230299           NaN   16.816322         High  Engineering\n",
       "4  37.658466  44733.108004   12.445495          NaN          NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic data with known relationships\n",
    "def create_example_data(n_rows=100):\n",
    "    \"\"\"Create example data with some missing values.\"\"\"\n",
    "    # Create a dataframe with some correlation between columns\n",
    "    df = pd.DataFrame({\n",
    "        'age': np.random.normal(40, 10, n_rows),\n",
    "        'income': np.random.normal(50000, 15000, n_rows),\n",
    "        'experience': np.random.normal(15, 7, n_rows),\n",
    "        'satisfaction': np.random.choice(['Low', 'Medium', 'High'], n_rows),\n",
    "        'department': np.random.choice(['HR', 'Engineering', 'Sales', 'Marketing', 'Support'], n_rows)\n",
    "    })\n",
    "    \n",
    "    # Add some correlations\n",
    "    df['experience'] = df['age'] * 0.4 + np.random.normal(0, 3, n_rows)\n",
    "    df['income'] = 20000 + df['experience'] * 2000 + np.random.normal(0, 5000, n_rows)\n",
    "    \n",
    "    # Add categorical biases\n",
    "    df.loc[df['department'] == 'Engineering', 'income'] += 10000\n",
    "    df.loc[df['department'] == 'Sales', 'income'] += 5000\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    df['satisfaction'] = df['satisfaction'].astype('category')\n",
    "    df['department'] = df['department'].astype('category')\n",
    "    \n",
    "    # Create a complete copy before adding missing values\n",
    "    df_complete = df.copy()\n",
    "    \n",
    "    # Add some missingness\n",
    "    mask = np.random.random(df.shape) < 0.15\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(df.shape[1]):\n",
    "            if mask[i, j]:\n",
    "                df.iloc[i, j] = np.nan\n",
    "    \n",
    "    return df, df_complete\n",
    "\n",
    "# Generate example data\n",
    "df_missing, df_complete = create_example_data(200)\n",
    "\n",
    "# Display the first few rows of the dataset with missing values\n",
    "print(\"First few rows of the dataset with missing values:\")\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Missing Data Patterns\n",
    "\n",
    "Before performing imputation, it's important to understand the pattern of missingness in the data. AutoFillGluon provides utility functions to analyze missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Missing Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Missing Percent",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Data Type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4e4f18ea-ed64-497c-b991-fbd946b2632e",
       "rows": [
        [
         "0",
         "age",
         "33",
         "16.5%",
         "float64"
        ],
        [
         "1",
         "income",
         "30",
         "15.0%",
         "float64"
        ],
        [
         "2",
         "experience",
         "44",
         "22.0%",
         "float64"
        ],
        [
         "3",
         "satisfaction",
         "21",
         "10.5%",
         "category"
        ],
        [
         "4",
         "department",
         "37",
         "18.5%",
         "category"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing Percent</th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>33</td>\n",
       "      <td>16.5%</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>income</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0%</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>experience</td>\n",
       "      <td>44</td>\n",
       "      <td>22.0%</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>satisfaction</td>\n",
       "      <td>21</td>\n",
       "      <td>10.5%</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>department</td>\n",
       "      <td>37</td>\n",
       "      <td>18.5%</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column  Missing Count Missing Percent Data Type\n",
       "0           age             33           16.5%   float64\n",
       "1        income             30           15.0%   float64\n",
       "2    experience             44           22.0%   float64\n",
       "3  satisfaction             21           10.5%  category\n",
       "4    department             37           18.5%  category"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and display missingness statistics\n",
    "missing_stats = calculate_missingness_statistics(df_missing)\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary = pd.DataFrame([{\n",
    "    'Column': col,\n",
    "    'Missing Count': stats['count_missing'],\n",
    "    'Missing Percent': f\"{stats['percent_missing']:.1f}%\",\n",
    "    'Data Type': df_missing[col].dtype\n",
    "} for col, stats in missing_stats.items()])\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imputation\n",
    "\n",
    "Now we'll use the `Imputer` class to fill in the missing values in our dataset. The imputer will train a separate machine learning model for each column with missing values, using the other columns as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 13:34:22,176 - autofillgluon.imputer.imputer - INFO - Fitting the imputer to the data...\n",
      "2025-04-06 13:34:22,188 - autofillgluon.imputer.imputer - INFO - Iteration 1/2\n",
      "2025-04-06 13:34:22,190 - autofillgluon.imputer.imputer - INFO - Processing column: department\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == category).\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the imputer to the data...\n",
      "Iteration 1/2\n",
      "Processing column: department\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize imputer with conservative settings for this example\n",
    "imputer = Imputer(\n",
    "    num_iter=2,          # Number of iterations for imputation refinement\n",
    "    time_limit=20,       # Time limit per column model (seconds)\n",
    "    verbose=True         # Show progress information\n",
    ")\n",
    "\n",
    "# Fit imputer on data with missing values\n",
    "df_imputed = imputer.fit(df_missing)\n",
    "\n",
    "# Display the first few rows of the imputed dataset\n",
    "print(\"First few rows of the imputed dataset:\")\n",
    "df_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Imputation Quality\n",
    "\n",
    "Since we have the original complete data, we can evaluate how well our imputation performed by comparing the imputed values with the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numeric columns, we'll calculate correlation and error metrics\n",
    "numeric_cols = ['age', 'income', 'experience']\n",
    "numeric_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    # Find indices with missing values in the original data\n",
    "    missing_mask = df_missing[col].isnull()\n",
    "    if missing_mask.sum() > 0:\n",
    "        # Get true and imputed values\n",
    "        true_vals = df_complete.loc[missing_mask, col]\n",
    "        imputed_vals = df_imputed.loc[missing_mask, col]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        corr = np.corrcoef(true_vals, imputed_vals)[0, 1]\n",
    "        mae = np.abs(true_vals - imputed_vals).mean()\n",
    "        mse = ((true_vals - imputed_vals) ** 2).mean()\n",
    "        \n",
    "        numeric_results.append({\n",
    "            'Column': col,\n",
    "            'Correlation': f\"{corr:.4f}\",\n",
    "            'MAE': f\"{mae:.4f}\",\n",
    "            'MSE': f\"{mse:.4f}\"\n",
    "        })\n",
    "\n",
    "# Display numeric results\n",
    "pd.DataFrame(numeric_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical columns, we'll calculate accuracy\n",
    "categorical_cols = ['satisfaction', 'department']\n",
    "categorical_results = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Find indices with missing values in the original data\n",
    "    missing_mask = df_missing[col].isnull()\n",
    "    if missing_mask.sum() > 0:\n",
    "        # Get true and imputed values\n",
    "        true_vals = df_complete.loc[missing_mask, col]\n",
    "        imputed_vals = df_imputed.loc[missing_mask, col]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = (true_vals == imputed_vals).mean()\n",
    "        \n",
    "        categorical_results.append({\n",
    "            'Column': col,\n",
    "            'Accuracy': f\"{accuracy:.4f}\",\n",
    "            'Correct': f\"{(true_vals == imputed_vals).sum()}/{len(true_vals)}\"\n",
    "        })\n",
    "\n",
    "# Display categorical results\n",
    "pd.DataFrame(categorical_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imputation results for a numeric column (age)\n",
    "missing_mask = df_missing['age'].isnull()\n",
    "if missing_mask.sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df_complete.loc[missing_mask, 'age'], \n",
    "                df_imputed.loc[missing_mask, 'age'], \n",
    "                alpha=0.7)\n",
    "    plt.plot([df_complete['age'].min(), df_complete['age'].max()], \n",
    "            [df_complete['age'].min(), df_complete['age'].max()], \n",
    "            'r--')\n",
    "    \n",
    "    # Add regression line\n",
    "    sns.regplot(x=df_complete.loc[missing_mask, 'age'], \n",
    "                y=df_imputed.loc[missing_mask, 'age'], \n",
    "                scatter=False, color='blue')\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    corr = np.corrcoef(df_complete.loc[missing_mask, 'age'], df_imputed.loc[missing_mask, 'age'])[0, 1]\n",
    "    plt.text(0.05, 0.95, f'Correlation: {corr:.4f}', \n",
    "            transform=plt.gca().transAxes, fontsize=12)\n",
    "    \n",
    "    plt.xlabel('True Age')\n",
    "    plt.ylabel('Imputed Age')\n",
    "    plt.title('True vs Imputed Values for Age')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Built-in Evaluation Function\n",
    "\n",
    "The `Imputer` class includes a built-in `evaluate` method that can assess imputation performance by artificially introducing missingness into complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built-in evaluation function\n",
    "eval_results = imputer.evaluate(\n",
    "    data=df_complete,        # Complete data without missing values\n",
    "    percentage=0.15,         # Percentage to set as missing\n",
    "    n_samples=3              # Number of evaluation samples\n",
    ")\n",
    "\n",
    "# Format results for display\n",
    "evaluation_summary = []\n",
    "for col, metrics in eval_results.items():\n",
    "    for metric_name, values in metrics.items():\n",
    "        evaluation_summary.append({\n",
    "            'Column': col,\n",
    "            'Metric': metric_name,\n",
    "            'Mean': f\"{values['mean']:.4f}\",\n",
    "            'Std Dev': f\"{values['std']:.4f}\",\n",
    "            'Min': f\"{values['min']:.4f}\",\n",
    "            'Max': f\"{values['max']:.4f}\"\n",
    "        })\n",
    "\n",
    "pd.DataFrame(evaluation_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading the Imputer\n",
    "\n",
    "After training an imputer, you can save it for later use on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the imputer\n",
    "imputer.save('example_imputer')\n",
    "\n",
    "# Load the imputer back\n",
    "loaded_imputer = Imputer.load('example_imputer')\n",
    "\n",
    "# Create new data with missing values\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [35, np.nan, 42, 55, np.nan],\n",
    "    'income': [np.nan, 45000, 60000, np.nan, 75000],\n",
    "    'experience': [10, 15, np.nan, 25, 20],\n",
    "    'satisfaction': ['Medium', np.nan, 'High', 'Low', 'Medium'],\n",
    "    'department': [np.nan, 'Engineering', 'Sales', 'HR', np.nan]\n",
    "})\n",
    "\n",
    "# Apply loaded imputer to new data\n",
    "new_data_imputed = loaded_imputer.transform(new_data)\n",
    "\n",
    "# Display results\n",
    "pd.concat([new_data.add_suffix('_original'), new_data_imputed.add_suffix('_imputed')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple Imputation\n",
    "\n",
    "Multiple imputation creates several imputed datasets to account for the uncertainty in imputation. This approach allows for more robust statistical inference when working with imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multiple imputation\n",
    "imputed_datasets = multiple_imputation(\n",
    "    data=df_missing,       # Data with missing values\n",
    "    n_imputations=3,       # Number of imputed datasets to create\n",
    "    fitonce=True,          # Fit one model and use it for all imputations\n",
    "    num_iter=2,            # Number of iterations for each imputation\n",
    "    time_limit=15,         # Time limit per column model (seconds)\n",
    "    verbose=True           # Show progress information\n",
    ")\n",
    "\n",
    "# Compare the first row of imputed values across datasets\n",
    "comparison = pd.DataFrame({\n",
    "    f\"Imputation {i+1}\": dataset.iloc[0] \n",
    "    for i, dataset in enumerate(imputed_datasets)\n",
    "})\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Multiple Imputation Results\n",
    "\n",
    "Here we'll see how imputed values can vary across multiple imputations and how to incorporate this uncertainty into analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at imputed values for a specific variable across datasets\n",
    "col_to_analyze = 'income'\n",
    "\n",
    "# Find rows with missing values for this column\n",
    "missing_mask = df_missing[col_to_analyze].isnull()\n",
    "missing_indices = df_missing.index[missing_mask]\n",
    "\n",
    "if len(missing_indices) > 0:\n",
    "    # Compare imputed values across datasets\n",
    "    imputed_values = pd.DataFrame({\n",
    "        f\"Imputation {i+1}\": dataset.loc[missing_indices, col_to_analyze]\n",
    "        for i, dataset in enumerate(imputed_datasets)\n",
    "    })\n",
    "    \n",
    "    # Add the true values if available\n",
    "    imputed_values['True Value'] = df_complete.loc[missing_indices, col_to_analyze]\n",
    "    \n",
    "    # Calculate mean and standard deviation across imputations\n",
    "    imputed_values['Mean'] = imputed_values.iloc[:, :-1].mean(axis=1)\n",
    "    imputed_values['Std Dev'] = imputed_values.iloc[:, :-1].std(axis=1)\n",
    "    \n",
    "    # Display the results\n",
    "    imputed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variation in imputed values for a numeric column\n",
    "if len(missing_indices) > 0:\n",
    "    # Create a boxplot of imputed values across datasets\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get data for plotting\n",
    "    plot_data = pd.melt(\n",
    "        imputed_values.iloc[:, :-3], \n",
    "        var_name='Imputation', \n",
    "        value_name='Imputed Value'\n",
    "    )\n",
    "    \n",
    "    # Add true values as a separate column\n",
    "    plot_data['True Value'] = np.repeat(imputed_values['True Value'].values, len(imputed_datasets))\n",
    "    \n",
    "    # Create boxplot\n",
    "    sns.boxplot(data=plot_data, x='Imputation', y='Imputed Value')\n",
    "    \n",
    "    # Overlay true values\n",
    "    for i, imp in enumerate(plot_data['Imputation'].unique()):\n",
    "        subset = plot_data[plot_data['Imputation'] == imp]\n",
    "        plt.scatter(\n",
    "            x=[i] * len(subset), \n",
    "            y=subset['True Value'], \n",
    "            color='red', \n",
    "            marker='x', \n",
    "            s=50, \n",
    "            label='True Value' if i == 0 else None\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('Imputation Dataset')\n",
    "    plt.ylabel(f'Imputed Value for {col_to_analyze}')\n",
    "    plt.title(f'Variation in Imputed Values Across Multiple Imputations\\n(Red X = True Value)')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Survival Analysis with AutoFillGluon\n",
    "\n",
    "AutoFillGluon includes custom scoring functions for survival analysis. In this section, we'll demonstrate how to use these scorers with the lifelines Rossi recidivism dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Rossi recidivism dataset from lifelines\n",
    "from lifelines.datasets import load_rossi\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "rossi = load_rossi()\n",
    "\n",
    "# Ensure time column is float\n",
    "rossi['week'] = rossi['week'].astype(float)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Dataset shape: {rossi.shape}\")\n",
    "print(\"\\nColumn descriptions:\")\n",
    "print(\"- week: Week of first arrest after release or end of study\")\n",
    "print(\"- arrest: Arrested during study period? (1=yes, 0=no)\")\n",
    "print(\"- fin: Financial aid received? (1=yes, 0=no)\")\n",
    "print(\"- age: Age at release (years)\")\n",
    "print(\"- race: Race (1=black, 0=other)\")\n",
    "print(\"- wexp: Work experience (1=yes, 0=no)\")\n",
    "print(\"- mar: Married? (1=yes, 0=no)\")\n",
    "print(\"- paro: Released on parole? (1=yes, 0=no)\")\n",
    "print(\"- prio: Number of prior convictions\")\n",
    "\n",
    "# Show the first few rows\n",
    "rossi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the survival data for AutoGluon\n",
    "def prepare_survival_data(df, time_col, event_col):\n",
    "    \"\"\"Prepare survival data by encoding time and event in a single column.\"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Create the time column (positive for events, negative for censored)\n",
    "    df_model['time'] = df_model[time_col]\n",
    "    # Encode censored observations with negative times\n",
    "    df_model.loc[df_model[event_col] == 0, 'time'] = -df_model.loc[df_model[event_col] == 0, time_col]\n",
    "    \n",
    "    # Drop the original time and event columns\n",
    "    df_model = df_model.drop(columns=[time_col, event_col])\n",
    "    \n",
    "    return df_model\n",
    "\n",
    "# Prepare the dataset for AutoGluon\n",
    "df_model = prepare_survival_data(rossi, 'week', 'arrest')\n",
    "\n",
    "# Create a version with artificial missing values\n",
    "np.random.seed(42)\n",
    "mask = np.random.random(df_model.shape) < 0.15\n",
    "\n",
    "# Create a copy with missing values (don't introduce missingness in the target column)\n",
    "df_missing = df_model.copy()\n",
    "for i in range(df_missing.shape[0]):\n",
    "    for j in range(df_missing.shape[1]):\n",
    "        # Skip the target column (time)\n",
    "        if j != df_missing.columns.get_loc('time') and mask[i, j]:\n",
    "            df_missing.iloc[i, j] = np.nan\n",
    "\n",
    "# Show the missing data\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df_missing.isnull().sum())\n",
    "df_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation for Survival Data\n",
    "\n",
    "Let's first impute the missing values in our survival dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "imputer = Imputer(num_iter=2, time_limit=15, verbose=True)\n",
    "df_imputed = imputer.fit(df_missing)\n",
    "\n",
    "# Convert to TabularDataset for AutoGluon\n",
    "df_model = TabularDataset(df_model)\n",
    "df_imputed = TabularDataset(df_imputed)\n",
    "\n",
    "# Display imputation summary\n",
    "print(f\"Original shape: {df_model.shape}\")\n",
    "print(f\"Missing data shape: {df_missing.shape}\")\n",
    "print(f\"Imputed data shape: {df_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival Analysis with Custom Scoring Functions\n",
    "\n",
    "Now let's demonstrate the survival analysis capabilities of AutoFillGluon using various scoring functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for all models\n",
    "common_params = {\n",
    "    'label': 'time',       # The target variable\n",
    "    'time_limit': 60,      # Time limit in seconds\n",
    "    'presets': 'medium_quality',\n",
    "    'verbosity': 0         # Reduce verbosity\n",
    "}\n",
    "\n",
    "# Train with Cox PH scorer\n",
    "print(\"Training with Cox PH scorer...\")\n",
    "cox_predictor = TabularPredictor(eval_metric=cox_ph_scorer, **common_params)\n",
    "cox_predictor.fit(df_model)\n",
    "\n",
    "# Train with concordance index scorer\n",
    "print(\"Training with Concordance Index scorer...\")\n",
    "cindex_predictor = TabularPredictor(eval_metric=concordance_index_scorer, **common_params)\n",
    "cindex_predictor.fit(df_model)\n",
    "\n",
    "# Train with exponential NLL scorer\n",
    "print(\"Training with Exponential NLL scorer...\")\n",
    "exp_predictor = TabularPredictor(eval_metric=exponential_nll_scorer, **common_params)\n",
    "exp_predictor.fit(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "cox_preds = cox_predictor.predict(df_model)\n",
    "cindex_preds = cindex_predictor.predict(df_model)\n",
    "exp_preds = exp_predictor.predict(df_model)\n",
    "\n",
    "# Evaluate predictions using concordance index\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "def evaluate_predictions(y_true, y_true_event, y_pred):\n",
    "    \"\"\"Evaluate predictions using concordance index.\"\"\"\n",
    "    # For concordance_index, higher predictions should indicate higher risk\n",
    "    c_index = concordance_index(y_true, -y_pred, event_observed=y_true_event)\n",
    "    return c_index\n",
    "\n",
    "# Evaluate models\n",
    "cox_cindex = evaluate_predictions(rossi['week'], rossi['arrest'], cox_preds)\n",
    "cindex_cindex = evaluate_predictions(rossi['week'], rossi['arrest'], cindex_preds)\n",
    "exp_cindex = evaluate_predictions(rossi['week'], rossi['arrest'], exp_preds)\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Cox PH', 'Concordance Index', 'Exponential NLL'],\n",
    "    'C-index': [cox_cindex, cindex_cindex, exp_cindex]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models and visualize risk scores\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(rossi['week'], -cox_preds, c=rossi['arrest'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Event (1=arrest)')\n",
    "plt.xlabel('Time (weeks)')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.title('Cox PH Risk Scores')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(rossi['week'], -cindex_preds, c=rossi['arrest'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Event (1=arrest)')\n",
    "plt.xlabel('Time (weeks)')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.title('C-index Risk Scores')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(rossi['week'], -exp_preds, c=rossi['arrest'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Event (1=arrest)')\n",
    "plt.xlabel('Time (weeks)')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.title('Exponential NLL Risk Scores')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Imputation on Survival Models\n",
    "\n",
    "Let's compare model performance between complete data and imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on imputed data\n",
    "imputed_predictor = TabularPredictor(eval_metric=cox_ph_scorer, **common_params)\n",
    "imputed_predictor.fit(df_imputed)\n",
    "\n",
    "# Compare the leaderboards\n",
    "print(\"Original data leaderboard:\")\n",
    "original_leaderboard = cox_predictor.leaderboard(df_model, silent=True)[['model', 'score', 'pred_time_val']]\n",
    "print(original_leaderboard)\n",
    "\n",
    "print(\"\\nImputed data leaderboard:\")\n",
    "imputed_leaderboard = imputed_predictor.leaderboard(df_imputed, silent=True)[['model', 'score', 'pred_time_val']]\n",
    "print(imputed_leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this vignette, we've demonstrated the key features of AutoFillGluon:\n",
    "\n",
    "1. **Basic imputation** - Using the Imputer class to fill missing values with ML-based predictions\n",
    "2. **Evaluation** - Assessing imputation quality through various metrics\n",
    "3. **Multiple imputation** - Creating multiple imputed datasets to account for uncertainty\n",
    "4. **Survival analysis** - Using custom scoring functions to train survival models with AutoGluon\n",
    "\n",
    "AutoFillGluon provides a powerful toolkit for handling missing data in your machine learning projects, combining the ease of use of simple imputation methods with the predictive power of AutoGluon's machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScienceEnv_Autoencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
